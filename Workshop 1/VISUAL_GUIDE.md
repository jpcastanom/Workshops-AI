# ğŸ¨ MiniTorch Visual Guide

## Neural Network Architecture Diagram

```
INPUT (28Ã—28 image = 784 pixels)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Linear Layer (784 â†’ 256)      â”‚  â† Weights & Biases
â”‚   Z = X @ W + b                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Batch Normalization            â”‚  â† Normalize activations
â”‚   (mean=0, variance=1)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ReLU Activation                â”‚  â† Add non-linearity
â”‚   output = max(0, input)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dropout (p=0.2)                â”‚  â† Regularization
â”‚   Randomly zero 20% of neurons   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Linear Layer (256 â†’ 10)        â”‚  â† Output layer
â”‚   Z = X @ W + b                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
OUTPUT (10 class scores)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Softmax + Cross-Entropy        â”‚  â† Loss calculation
â”‚   Convert to probabilities       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
LOSS (single number)
```

## Forward and Backward Pass Flow

```
FORWARD PASS (Making Predictions)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Input Image (784 pixels)
    â”‚
    â”œâ”€â†’ Linear Layer
    â”‚   â€¢ Multiply by weights
    â”‚   â€¢ Add bias
    â”‚   â€¢ Output: 256 numbers
    â”‚
    â”œâ”€â†’ BatchNorm
    â”‚   â€¢ Normalize to mean=0, var=1
    â”‚   â€¢ Scale and shift
    â”‚
    â”œâ”€â†’ ReLU
    â”‚   â€¢ Keep positive, zero negative
    â”‚
    â”œâ”€â†’ Dropout
    â”‚   â€¢ Randomly zero some neurons
    â”‚
    â”œâ”€â†’ Linear Layer
    â”‚   â€¢ Output: 10 numbers (one per class)
    â”‚
    â””â”€â†’ Loss Function
        â€¢ Convert to probabilities (softmax)
        â€¢ Calculate error (cross-entropy)
        â€¢ Output: Loss (how wrong we are)


BACKWARD PASS (Learning from Mistakes)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Loss (error value)
    â”‚
    â”œâ”€â†’ Loss Gradient
    â”‚   â€¢ dZ = probabilities - true_labels
    â”‚
    â”œâ”€â†’ Linear Layer Backward
    â”‚   â€¢ Calculate dW, db (how to change weights)
    â”‚   â€¢ Pass gradient to previous layer
    â”‚
    â”œâ”€â†’ Dropout Backward
    â”‚   â€¢ Gradient flows through same mask
    â”‚
    â”œâ”€â†’ ReLU Backward
    â”‚   â€¢ Gradient flows where input > 0
    â”‚
    â”œâ”€â†’ BatchNorm Backward
    â”‚   â€¢ Calculate gradients for gamma, beta
    â”‚   â€¢ Pass gradient to previous layer
    â”‚
    â””â”€â†’ Linear Layer Backward
        â€¢ Calculate dW, db
        â€¢ (No need to pass further back)


UPDATE (Adjusting Weights)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

For each layer with parameters:
    W_new = W_old - learning_rate Ã— dW
    b_new = b_old - learning_rate Ã— db
```

## Matrix Dimensions Flow

```
MNIST Example (batch_size = 64)

Input:  [64, 784]  (64 images, 784 pixels each)
           â†“
Linear: [64, 784] @ [784, 256] + [256]
           â†“
Output: [64, 256]  (64 samples, 256 features)
           â†“
BatchNorm: [64, 256] â†’ [64, 256]
           â†“
ReLU:   [64, 256] â†’ [64, 256]
           â†“
Dropout: [64, 256] â†’ [64, 256]
           â†“
Linear: [64, 256] @ [256, 10] + [10]
           â†“
Output: [64, 10]  (64 samples, 10 class scores)
           â†“
Softmax: [64, 10] â†’ [64, 10]  (probabilities)
           â†“
Loss:   [64, 10] + [64] labels â†’ scalar
```

## Gradient Flow (Backward Pass)

```
Loss (scalar)
    â†“
dZ: [64, 10]  â† Gradient of loss w.r.t. output
    â†“
Linear Layer:
    dW: [256, 10]  â† Gradient w.r.t. weights
    db: [10]       â† Gradient w.r.t. bias
    dX: [64, 256]  â† Gradient to pass back
    â†“
Dropout:
    dX: [64, 256]  â† Gradient through mask
    â†“
ReLU:
    dX: [64, 256]  â† Gradient where input > 0
    â†“
BatchNorm:
    dgamma: [256]  â† Gradient w.r.t. scale
    dbeta:  [256]  â† Gradient w.r.t. shift
    dX: [64, 256]  â† Gradient to pass back
    â†“
Linear Layer:
    dW: [784, 256] â† Gradient w.r.t. weights
    db: [256]      â† Gradient w.r.t. bias
    dX: [64, 784]  â† (Not needed, we're at input)
```

## Training Loop Visualization

```
EPOCH 1
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Batch 1:  [Forward] â†’ [Loss: 2.30] â†’ [Backward] â†’ [Update]
Batch 2:  [Forward] â†’ [Loss: 2.15] â†’ [Backward] â†’ [Update]
Batch 3:  [Forward] â†’ [Loss: 1.98] â†’ [Backward] â†’ [Update]
...
Batch N:  [Forward] â†’ [Loss: 0.85] â†’ [Backward] â†’ [Update]

Validation: [Forward only] â†’ [Loss: 0.92, Acc: 72%]

EPOCH 2
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Batch 1:  [Forward] â†’ [Loss: 0.78] â†’ [Backward] â†’ [Update]
Batch 2:  [Forward] â†’ [Loss: 0.71] â†’ [Backward] â†’ [Update]
...

Validation: [Forward only] â†’ [Loss: 0.45, Acc: 87%]

...

EPOCH 10
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Validation: [Forward only] â†’ [Loss: 0.12, Acc: 97%]

âœ… Training Complete!
```

## How Gradient Descent Works

```
Imagine you're on a hill and want to reach the valley (minimum loss):

Current Position (Loss = 2.5)
        ğŸ”ï¸
       /  \
      /    \
     /      \
    /        \
   /          \
  /            \
 /              \
ğŸš¶ â† You are here

Step 1: Calculate gradient (which way is down?)
        â†“ (gradient points down)

Step 2: Take a step in that direction
        learning_rate controls step size

After Update (Loss = 2.1)
        ğŸ”ï¸
       /  \
      /    \
     /      \
    /   ğŸš¶   \  â† You moved down!
   /          \
  /            \
 /              \

Repeat many times...

Final Position (Loss = 0.1)
        ğŸ”ï¸
       /  \
      /    \
     /      \
    /        \
   /          \
  /            \
 /      ğŸš¶      \  â† Reached the valley!
```

## Batch Normalization Effect

```
WITHOUT BATCH NORMALIZATION:
Layer 1 output: [-100, 50, 200, -80, ...]  â† Unstable!
Layer 2 output: [-1000, 500, 2000, ...]    â† Getting worse!
Layer 3 output: [NaN, NaN, NaN, ...]       â† Exploded! ğŸ’¥

WITH BATCH NORMALIZATION:
Layer 1 output: [-1.2, 0.5, 2.0, -0.8, ...]  â† Normalized!
Layer 2 output: [-0.9, 0.3, 1.5, -0.6, ...]  â† Still stable!
Layer 3 output: [-1.1, 0.4, 1.8, -0.7, ...]  â† Working! âœ…
```

## Dropout Visualization

```
TRAINING MODE (Dropout p=0.5):

Before Dropout:
[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]

Random Mask (50% kept):
[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]

After Dropout (scaled by 1/0.5 = 2):
[2.0, 0.0, 6.0, 0.0, 10.0, 0.0, 14.0, 0.0]
 âœ“    âœ—    âœ“    âœ—     âœ“     âœ—     âœ“     âœ—

INFERENCE MODE:
[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]
(No dropout, all neurons active)
```

## Loss Curve (What Success Looks Like)

```
Loss
 â”‚
3â”‚ â—
 â”‚  â—
2â”‚   â—â—
 â”‚     â—â—
1â”‚       â—â—â—
 â”‚          â—â—â—â—
0â”‚              â—â—â—â—â—â—â—â—â—â—
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs
  1  2  3  4  5  6  7  8  9  10

â— = Training Loss
â—‹ = Validation Loss

Good signs:
âœ… Both losses decrease
âœ… Validation follows training closely
âœ… Smooth curve

Bad signs:
âŒ Loss increases
âŒ Validation much higher than training (overfitting)
âŒ Erratic jumps
```

## Accuracy Curve (What Success Looks Like)

```
Accuracy (%)
 â”‚
100â”‚                    â—â—â—â—â—â—â—â—
 90â”‚              â—â—â—â—â—â—
 80â”‚         â—â—â—â—â—
 70â”‚     â—â—â—â—
 60â”‚  â—â—â—
 50â”‚ â—
  0â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs
    1  2  3  4  5  6  7  8  9  10

â— = Training Accuracy
â—‹ = Validation Accuracy

Target for MNIST:
âœ… Training: 98-99%
âœ… Validation: 97-98%
```

## Memory Flow in a Layer

```
LINEAR LAYER MEMORY:

Stored during Forward (for Backward):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ self.X = input               â”‚  â† Need for dW calculation
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Calculated during Backward:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ self.dW = X.T @ dZ / m       â”‚  â† Gradient for weights
â”‚ self.db = sum(dZ) / m        â”‚  â† Gradient for bias
â”‚ self.dX = dZ @ W.T           â”‚  â† Gradient to pass back
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Used during Update:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ W = W - lr Ã— dW              â”‚  â† Update weights
â”‚ b = b - lr Ã— db              â”‚  â† Update bias
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Complete Example: One Training Step

```
INPUT: Image of digit "3"
[0.1, 0.2, ..., 0.9]  (784 pixels)

FORWARD:
Linear1:  [784] â†’ [256]
BatchNorm: normalize
ReLU:     keep positive
Dropout:  random mask
Linear2:  [256] â†’ [10]

OUTPUT: [0.1, 0.05, 0.08, 0.7, 0.02, 0.01, 0.01, 0.02, 0.01, 0.0]
         0    1     2     3    4     5     6     7     8     9
                          â†‘
                    Predicted: 3 âœ…

LOSS: -log(0.7) = 0.36  (pretty good!)

BACKWARD:
Calculate how to adjust all weights to make 0.7 â†’ 1.0

UPDATE:
Adjust weights slightly in the right direction

NEXT IMAGE: Repeat!
```

## Summary: The Big Picture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                 â”‚
â”‚  INPUT â†’ LAYERS â†’ OUTPUT â†’ LOSS                â”‚
â”‚           â†‘                    â†“                â”‚
â”‚           â”‚                    â”‚                â”‚
â”‚           â””â”€â”€â”€â”€ GRADIENTS â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                 â”‚
â”‚  Repeat thousands of times...                  â”‚
â”‚  Network gradually learns patterns!            â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Key Takeaways

1. **Forward Pass**: Data flows through layers to make predictions
2. **Loss**: Measures how wrong predictions are
3. **Backward Pass**: Calculates how to improve (gradients)
4. **Update**: Adjusts weights to reduce error
5. **Repeat**: Network learns through repetition

## ğŸ’¡ Remember

- Shapes matter! Always check dimensions
- Gradients flow backward through the same path
- Learning rate controls how big each step is
- Batch size affects gradient averaging

Good luck with your workshop! ğŸš€
